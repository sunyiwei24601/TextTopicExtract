# 基于TFIDF的文本主题分析

​	本项目爬取了机器之心网站[新闻快讯](https://www.jiqizhixin.com/dailies)的内容，先用jieba分词，通过TF-IDF文本分析的方式，将一年的新闻作为语料库，来分析某一周的文章主题关键词以及热门主题。

## 数据爬取

​	通过对chrome后台网络的分析，发现机器之心网站后台采用的是Graphql框架进行数据传输，并无其他爬虫限制，所以采用最简单的requests.post获取数据。比较麻烦的是需要伪装浏览器的headers，在这里我直接从chrome后台进行复制，cookie似乎是网站识别发送方的主要方式，每隔一段时间就会失效。

​	graphql架构的关键在于翻页时候需要用cursor来指定下一页开始的位置，所幸网站的接口提供了pageInfo的内容，其中就有endcursor。所以只需要每次爬取一页，获取endcursor，作为下一页post数据的after cursor就可以了。

​	总新闻数为9000篇左右，有趣的是，超过9000篇会自动从最新的开始提供，应该是endcursor又回到了开头的缘故。最后将数据存储在“dailies.json"中。

## 主题提取

​	一般来说主题提取有两种方式，一种是TF-IDF，另一种是lDA相关的模型。由于LDA分词结果是类别的分数，而不是以关键词作为分数，**为了得到满足题意的效果，我选择了TF-IDF模型**。

### jieba分词

​	直接调用jieba分词的包将文章进行分词，这里有三种分词模式，精准搜索模式容易带来冗余信息，所以这里我们选择`jiebe.cut(data, cut_all=False)`更好。

​	jieba分词并没有去停用词的功能，这里我们手动下载了”中文停词库.txt"，再加上文中过多出现的一些词汇和标点符号进行去除，如'「', '」'等。

### TF-IDF计算

​	我将一年的新闻作为计算TF-IDF的基础语料库，这样计算出的IDF会比普通包调用的IDF更加合理。这里我适用的是gensim包的tfidf模型也更好地存储何使用。

​	**由于TF-IDF无法考量词所在的位置，我选择将文章的标题一同纳入考量**。在将文本转化为词向量的时候，标题的权重是普通文本的三倍，以运用到之后所有的计算中。

​	最后tfidf的模型存储在“model.tfidf"文件中，方便适用。

### 一周文章的主题词计算

​	首先根据时间戳，获取2019-2-20到2019-2-27这一个礼拜的文章序号。

​	再根据他们的title和content计算得到相应的tfidf值。

​	最后按照tfidf值降序排列，选取最大的十个词，并用dictionary映射出他们对应的中文，存储在`article_topics.json`中。

### 一周热门主题

​	这里我们就简单的将一周以内所有文章的所有词的tfidf结果现加，降序排列得到前十的结果，存储在文件`week_topic_words.json`中。

##  其他

​	其实还应该可以做一些词与词之间的近义词去重之类的，这里没有想好，所以就没做。

​	如果发在Github上导致泄题的话，请联系我删除。

